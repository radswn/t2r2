{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook explaining the nuts and bots of data cartography implementation in enginora"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca50676e09e29ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of the concept of data cartography as outlined in the paper: https://aclanthology.org/2020.emnlp-main.746/. As the authors write:\n",
    "\n",
    " *Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: thebehavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example—the model’s confidence in the true class, and the variability ofthis confidence across epochs—obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ambiguous regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are easy to learn for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds hard to learn; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f19abf097cbc71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: \n",
    "Run Data Cartography, in the training config, set perform_data_cartography to True, and make sure you provide a distinict validatoon set, so that data cartography can be conducted on the whole of the training set. You can set a file destination for the data cartography results.\n",
    "Make sure it looks something like this:\n",
    "\n",
    "```\n",
    "training:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.train.txt\n",
    "  validation_dataset_path: ../../data/featuresets/thedeep.subset.validation.txt\n",
    "  selectors:\n",
    "    - args: { }\n",
    "      name: dummy\n",
    "  batch_size: 2\n",
    "  epochs: 2\n",
    "  learning_rate: 1e-3\n",
    "  validation_size: 0.2\n",
    "  output_dir: ClassificationBERT # set data_seed\n",
    "  metric_for_best_model: f1_score\n",
    "  perform_data_cartography: True\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de3735df54c193f1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import enginora\n",
    "import logging\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T11:37:27.368764200Z",
     "start_time": "2023-10-09T11:37:14.843364100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "logging.basicConfig(format=\"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\", level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T11:37:27.430258800Z",
     "start_time": "2023-10-09T11:37:27.380358500Z"
    }
   },
   "id": "ccd927f0436ad80e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\ismyn\\Anaconda3\\envs\\enginora\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/5000 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enginora.loop(config_path=\"./config_run_data_cartography.yaml\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-09T11:37:27.406877200Z"
    }
   },
   "id": "bbb7fb3b8ae3fd58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2:Perform experiments on subsets. \n",
    "\n",
    "You can now perform experiments: sampling data directly from the most ambiguous examples, the hard-to-learn examples and the easy-to-learn examples, You can also add randomly sapled fraction of data. The data will not allow for sampling with replacement.\n",
    "\n",
    "\n",
    "Ensure now your config has perform_data_cartography set to False, and the data cartography selector is enabled!\n",
    "\n",
    "```\n",
    "\n",
    "training:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.train.txt\n",
    "  validation_dataset_path: ../../data/featuresets/thedeep.subset.validation.txt\n",
    "  selectors:\n",
    "    - args: {\n",
    "              hard_to_learn: 0,\n",
    "              easy-to-learn: 0.3,\n",
    "              ambiguous: 0.1,\n",
    "              random: 0.6\n",
    "\n",
    "    }\n",
    "      name: data_cartography\n",
    "  batch_size: 2\n",
    "  epochs: 2\n",
    "  learning_rate: 1e-3\n",
    "  validation_size: 0.2\n",
    "  output_dir: ClassificationBERT # set data_seed\n",
    "  metric_for_best_model: f1_score\n",
    "\n",
    "testing:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.test.txt\n",
    "  selectors: [ ]\n",
    "  results_file: ./file_test_results_experiment.pickle\n",
    "\n",
    "control:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.control.txt\n",
    "  results_file: ./file_control_results_experiment.pickle\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe992894c3bbb4f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\ismyn/.cache\\huggingface\\transformers\\092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\ismyn\\Anaconda3\\envs\\enginora\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EASY INDCICES: 9 Int64Index([16, 6, 13], dtype='int64')\n",
      "HARD INDCICES: 0 []\n",
      "AMBIGUOUS INDCICES: 3 Int64Index([0, 9, 27, 25, 18, 16, 14, 13, 10], dtype='int64')\n",
      "RANDOM INDCICES: Int64Index([0, 5, 20], dtype='int64')\n",
      "indices: [0, 5, 6, 9, 10, 13, 14, 16, 18, 20, 25, 27]\n",
      "DATASET:        id                                               text  label\n",
      "0    5446  In addition to the immediate life-saving inter...      9\n",
      "5    8318  Secretary-General Antonio Guterres says in a d...      9\n",
      "6    3464  Safe access to basic services and adequate liv...      9\n",
      "9   12607  In the refugee camps of Nord and Sud Ubangi, a...      4\n",
      "10   1619  The Bidi Bidi settlement has sprung up at an e...      9\n",
      "13   9421  The Interior Ministry’s regular police and its...      9\n",
      "14   6027  The situation is especially bad for communitie...      4\n",
      "16  15671  Security forces used unlawful force and threat...      9\n",
      "18  10090  t the same time, too many girls marry and beco...      9\n",
      "20   4620  More emergency supplies including bottled wate...      1\n",
      "25   7290  Since 2014, the incidence of malaria in Burund...      4\n",
      "27   7224  2,300 Estimated number of UAC currently in Gre...      9\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/36 : < :, Epoch 0.17/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-6\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-6\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-6\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-12\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-12\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-12\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-18\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-18\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-18\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-24\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-24\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-24\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-30\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-30\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-30\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ClassificationBERT_experiment\\checkpoint-36\n",
      "Configuration saved in ClassificationBERT_experiment\\checkpoint-36\\config.json\n",
      "Model weights saved in ClassificationBERT_experiment\\checkpoint-36\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ClassificationBERT_experiment\\checkpoint-6 (score: 0.06593406593406594).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/5 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'train_results': {'train_runtime': 123.4428,\n  'train_samples_per_second': 0.583,\n  'train_steps_per_second': 0.292,\n  'total_flos': 9472848445440.0,\n  'train_loss': 1.4648439354366727,\n  'epoch': 6.0},\n 'test_results': {'test_loss': 3.715395450592041,\n  'test_f1_score': 0.030303030303030304,\n  'test_runtime': 3.7093,\n  'test_samples_per_second': 2.696,\n  'test_steps_per_second': 1.348},\n 'control_results': {'test_loss': 5.031211853027344,\n  'test_f1_score': 0.06666666666666668,\n  'test_runtime': 3.9379,\n  'test_samples_per_second': 2.539,\n  'test_steps_per_second': 1.27}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enginora.loop(config_path=\"./config_experiments_from_data_cartography_results.yaml\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T09:13:29.206692300Z",
     "start_time": "2023-10-09T09:11:10.654136Z"
    }
   },
   "id": "bf1d6d5e51c56bc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "you can compare it with the performance on regular data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2f4aff5e3e973a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d983329f8c4c8f13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
