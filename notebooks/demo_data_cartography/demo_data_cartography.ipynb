{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook explaining the nuts and bots of data cartography implementation in enginora"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca50676e09e29ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of the concept of data cartography as outlined in the paper: https://aclanthology.org/2020.emnlp-main.746/. As the authors write:\n",
    "\n",
    " *Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: thebehavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example—the model’s confidence in the true class, and the variability ofthis confidence across epochs—obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ambiguous regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are easy to learn for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds hard to learn; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f19abf097cbc71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: \n",
    "Run Data Cartography, in the training config, set perform_data_cartography to True, and make sure you provide a distinict validatoon set, so that data cartography can be conducted on the whole of the training set. You can set a file destination for the data cartography results.\n",
    "Make sure it looks something like this:\n",
    "\n",
    "```\n",
    "training:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.train.txt\n",
    "  validation_dataset_path: ../../data/featuresets/thedeep.subset.validation.txt\n",
    "  selectors:\n",
    "    - args: { }\n",
    "      name: dummy\n",
    "  batch_size: 2\n",
    "  epochs: 2\n",
    "  learning_rate: 1e-3\n",
    "  validation_size: 0.2\n",
    "  output_dir: ClassificationBERT # set data_seed\n",
    "  metric_for_best_model: f1_score\n",
    "  perform_data_cartography: True\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de3735df54c193f1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import enginora\n",
    "import logging\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:51:35.443142100Z",
     "start_time": "2023-10-08T21:51:21.622440100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "logging.basicConfig(format=\"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\", level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-08T21:51:35.479458600Z",
     "start_time": "2023-10-08T21:51:35.446666300Z"
    }
   },
   "id": "ccd927f0436ad80e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\ismyn\\Anaconda3\\envs\\enginora\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/100 : < :, Epoch 0.02/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results/checkpoint-50\n",
      "Configuration saved in ./results/checkpoint-50\\config.json\n",
      "Model weights saved in ./results/checkpoint-50\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100\\config.json\n",
      "Model weights saved in ./results/checkpoint-100\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-50 (score: 0.025974025974025976).\n",
      "[data_cartography.py:69 - compute_data_cartography_metrics() ] Computing training dynamics across 2 epochs\n",
      "[data_cartography.py:70 - compute_data_cartography_metrics() ] Metrics computed: confidence, variability, correctness, forgetfulness, threshold_closeness\n",
      "100%|██████████| 100/100 [00:00<00:00, 1251.23it/s]\n",
      "[category.py:223 -               update() ] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "[category.py:223 -               update() ] Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/50 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enginora.loop(config_path=\"./config_run_data_cartography.yaml\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-08T21:51:35.476702700Z"
    }
   },
   "id": "bbb7fb3b8ae3fd58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2:Perform experiments on subsets. \n",
    "\n",
    "You can now perform experiments: sampling data directly from the most ambiguous examples, the hard-to-learn examples and the easy-to-learn examples, You can also add randomly sapled fraction of data. The data will not allow for sampling with replacement.\n",
    "\n",
    "\n",
    "Ensure now your config has perform_data_cartography set to False, and the data cartography selector is enabled!\n",
    "\n",
    "```\n",
    "\n",
    "training:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.train.txt\n",
    "  validation_dataset_path: ../../data/featuresets/thedeep.subset.validation.txt\n",
    "  selectors:\n",
    "    - args: {\n",
    "              hard_to_learn: 0,\n",
    "              easy-to-learn: 0.3,\n",
    "              ambiguous: 0.1,\n",
    "              random: 0.6\n",
    "\n",
    "    }\n",
    "      name: data_cartography\n",
    "  batch_size: 2\n",
    "  epochs: 2\n",
    "  learning_rate: 1e-3\n",
    "  validation_size: 0.2\n",
    "  output_dir: ClassificationBERT # set data_seed\n",
    "  metric_for_best_model: f1_score\n",
    "\n",
    "testing:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.test.txt\n",
    "  selectors: [ ]\n",
    "  results_file: ./file_test_results_experiment.pickle\n",
    "\n",
    "control:\n",
    "  dataset_path: ../../data/featuresets/thedeep.subset.control.txt\n",
    "  results_file: ./file_control_results_experiment.pickle\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe992894c3bbb4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enginora.loop(config_path=\"./config_experiments_from_data_cartography_results.yaml\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf1d6d5e51c56bc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
