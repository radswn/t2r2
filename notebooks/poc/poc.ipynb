{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:45:40.790607Z",
     "start_time": "2023-04-19T18:45:37.552500Z"
    },
    "id": "bHhIkms-Z0QY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy, AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str\n",
    "    num_labels: int\n",
    "    max_length: int\n",
    "    truncation: bool\n",
    "    padding: str\n",
    "    return_tensors: str\n",
    "    output_attentions: bool\n",
    "    output_hidden_states: bool\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.num_labels = int(self.num_labels)\n",
    "        self.max_length = int(self.max_length)\n",
    "        self.truncation = bool(self.truncation)\n",
    "        self.output_attentions = bool(self.output_attentions)\n",
    "        self.output_hidden_states = bool(self.output_hidden_states)\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        return lambda input_tokens: tokenizer(\n",
    "            input_tokens,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            truncation=self.truncation,\n",
    "            return_tensors=self.return_tensors\n",
    "        )\n",
    "\n",
    "    def create_model(self):\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=self.num_labels,\n",
    "            output_attentions=self.output_attentions,\n",
    "            output_hidden_states=self.output_hidden_states,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Selector(ABC):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select(self, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SelectorConfig(yaml.YAMLObject):\n",
    "    name: str\n",
    "    args: Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DummySelector(Selector):\n",
    "\n",
    "    def select(self, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SELECTORS = {\n",
    "    'DUMMY': DummySelector,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainSetConfig:\n",
    "    path: str\n",
    "    selectors: List[SelectorConfig]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.selectors = [SelectorConfig(**t) for t in self.selectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    dataset: TrainSetConfig\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    output_dir: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dataset = TrainSetConfig(**self.dataset)\n",
    "        self.batch_size = int(self.batch_size)\n",
    "        self.epochs = int(self.epochs)\n",
    "        self.learning_rate = float(self.learning_rate)\n",
    "\n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv(self.dataset.path, header=None, names=['id', 'text', 'label'])\n",
    "\n",
    "        for t in self.dataset.selectors:\n",
    "            selector = SELECTORS[t.name](**t.args)\n",
    "            df = selector.select(df)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FIXME: solarski\n",
    "# about the metrics - I'm not sure if we should have separate metrics for all steps (validation, test, control)\n",
    "# I would rather go with single metrics config, that would be common for all those steps.\n",
    "# Then we don't need strange test_set_select (etc.) methods, which are either way redundant since the metric are calculated\n",
    "# during call to trainer.predict().\n",
    "# Only thing that we are interested in is saving those metrics for test and control stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "METRIC_FUNCTIONS = {\n",
    "    'accuracy': accuracy_score,\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MetricsConfig:\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationSetConfig:\n",
    "    path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    dataset: ValidationSetConfig\n",
    "    batch_size: int\n",
    "    metrics: List[MetricsConfig]\n",
    "    metric_for_best_model: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dataset = ValidationSetConfig(**self.dataset)\n",
    "        self.batch_size = int(self.batch_size)\n",
    "        self.metrics = [MetricsConfig(**m) for m in self.metrics]\n",
    "\n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.dataset.path, header=None, names=['id', 'text', 'label'])\n",
    "\n",
    "    # TODO: rename ???\n",
    "    def validation_set_select(self, predictions) -> dict:\n",
    "        predictions, true_labels = predictions[0], predictions[1]\n",
    "        predictions = predictions[0].argmax(1)\n",
    "\n",
    "        return {\n",
    "            metric.name:\n",
    "                METRIC_FUNCTIONS[metric.name](true_labels, predictions)\n",
    "            for metric in self.metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_predictions(object, predictions_filename):\n",
    "    file = open(predictions_filename, 'wb')\n",
    "    pickle.dump(object, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_predictions(predictions_filename):\n",
    "    file = open(predictions_filename, 'rb')\n",
    "    results_depickled = pickle.load(file)\n",
    "    file.close()\n",
    "    return results_depickled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestSetConfig:\n",
    "    path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestingConfig:\n",
    "    dataset: TestSetConfig\n",
    "    metrics: List[MetricsConfig]\n",
    "    results_file: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dataset = TestSetConfig(**self.dataset)\n",
    "        self.metrics = [MetricsConfig(**m) for m in self.metrics]\n",
    "\n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.dataset.path, header=None, names=['id', 'text', 'label'])\n",
    "\n",
    "    # TODO: rename ???\n",
    "    def test_set_select(self) -> dict:\n",
    "        predictions = load_predictions(self.results_file)\n",
    "        predictions, true_labels = predictions[0], predictions[1]\n",
    "        predictions = predictions[0].argmax(1)\n",
    "\n",
    "        return {\n",
    "            metric.name:\n",
    "                METRIC_FUNCTIONS[metric.name](true_labels, predictions)\n",
    "            for metric in self.metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Control set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ControlSetConfig:\n",
    "    path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ControlConfig:\n",
    "    dataset: ControlSetConfig\n",
    "    metrics: List[MetricsConfig]\n",
    "    results_file: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dataset = ControlSetConfig(**self.dataset)\n",
    "        self.metrics = [MetricsConfig(**m) for m in self.metrics]\n",
    "\n",
    "    def load_dataset(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.dataset.path, header=None, names=['id', 'text', 'label'])\n",
    "\n",
    "    # TODO: rename ???\n",
    "    def control_set_select(self) -> dict:\n",
    "        predictions = load_predictions(self.results_file)\n",
    "        predictions, true_labels = predictions[0], predictions[1]\n",
    "        predictions = predictions[0].argmax(1)\n",
    "\n",
    "        return {\n",
    "            metric.name:\n",
    "                METRIC_FUNCTIONS[metric.name](true_labels, predictions)\n",
    "            for metric in self.metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./config.yaml', 'r') as stream:\n",
    "    configuration = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(model_name='bert-base-cased', num_labels=12, max_length=256, truncation=True, padding='max_length', return_tensors='pt', output_attentions=True, output_hidden_states=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = ModelConfig(**configuration['model'])\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingConfig(dataset=TrainSetConfig(path='../../data/featuresets/thedeep.subset.train.txt', selectors=[SelectorConfig(name='DUMMY', args={})]), batch_size=2, epochs=1, learning_rate=0.001, output_dir='ClassificationBERT')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_config = TrainingConfig(**configuration['training'])\n",
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValidationConfig(dataset=ValidationSetConfig(path='../../data/featuresets/thedeep.subset.validation.txt'), batch_size=2, metrics=[MetricsConfig(name='accuracy')], metric_for_best_model='accuracy')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_config = ValidationConfig(**configuration['validation'])\n",
    "validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestingConfig(dataset=TestSetConfig(path='../../data/featuresets/thedeep.subset.test.txt'), metrics=[MetricsConfig(name='accuracy')], results_file='./file_test_results.pickle')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config = TestingConfig(**configuration['testing'])\n",
    "test_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ControlConfig(dataset=ControlSetConfig(path='../../data/featuresets/thedeep.subset.control.txt'), metrics=[MetricsConfig(name='accuracy')], results_file='./file_control_results.pickle')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_config = ControlConfig(**configuration['control'])\n",
    "control_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# Notebook flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = model_config.create_tokenizer()\n",
    "model = model_config.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': training_config.load_dataset()[:100],\n",
    "    'validation': validation_config.load_dataset()[:10],\n",
    "    'test': test_config.load_dataset()[:10],\n",
    "    'control': control_config.load_dataset()[:10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:46:08.200413Z",
     "start_time": "2023-04-19T18:45:42.675059Z"
    },
    "id": "iD5AxkR6Z0Qk"
   },
   "outputs": [],
   "source": [
    "tokens = {\n",
    "    dataset_type: tokenizer(dataset['text'].tolist())\n",
    "    for dataset_type, dataset in data.items()\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    dataset_type: torch.tensor(dataset['label'].tolist())\n",
    "    for dataset_type, dataset in data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:46:08.214691Z",
     "start_time": "2023-04-19T18:46:08.204946Z"
    },
    "id": "CK-5-XYEZ0Qn"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, labels: torch.Tensor):\n",
    "        self.input_ids = tokens.input_ids\n",
    "        self.attention_mask = tokens.attention_mask\n",
    "        self.token_type_ids = tokens.token_type_ids\n",
    "        self.y = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[i],\n",
    "            'attention_mask': self.attention_mask[i],\n",
    "            'token_type_ids': self.token_type_ids[i],\n",
    "            'labels': self.y[i]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:46:08.262069Z",
     "start_time": "2023-04-19T18:46:08.217860Z"
    },
    "id": "KHoxwyr_Z0Qp"
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    dataset_type: TextDataset(tokens[dataset_type], labels[dataset_type])\n",
    "    for dataset_type in data.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:46:08.310346Z",
     "start_time": "2023-04-19T18:46:08.295318Z"
    },
    "id": "yBvqyfNSZ0Qu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=training_config.output_dir,\n",
    "    learning_rate=training_config.learning_rate,\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    logging_strategy=IntervalStrategy.EPOCH,\n",
    "    per_device_train_batch_size=training_config.batch_size,\n",
    "    per_device_eval_batch_size=validation_config.batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=validation_config.metric_for_best_model,\n",
    "    num_train_epochs=training_config.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T18:46:10.657969Z",
     "start_time": "2023-04-19T18:46:08.325499Z"
    },
    "id": "-fzFgT35Z0Qx"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['validation'],\n",
    "    compute_metrics=validation_config.validation_set_select,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ismyn\\Anaconda3\\envs\\enginora\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946c9174c3cb4b04a5a1d20d288b6e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5115, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e91612a35b41c5ab9d9fd68fa2184a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ClassificationBERT\\checkpoint-50\n",
      "Configuration saved in ClassificationBERT\\checkpoint-50\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1494522094726562, 'eval_accuracy': 0.1, 'eval_runtime': 4.3847, 'eval_samples_per_second': 2.281, 'eval_steps_per_second': 1.14, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ClassificationBERT\\checkpoint-50\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ClassificationBERT\\checkpoint-50 (score: 0.1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 125.0182, 'train_samples_per_second': 0.8, 'train_steps_per_second': 0.4, 'train_loss': 2.5115106201171873, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.5115106201171873, metrics={'train_runtime': 125.0182, 'train_samples_per_second': 0.8, 'train_steps_per_second': 0.4, 'train_loss': 2.5115106201171873, 'epoch': 1.0})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea91ceab2d949aab0b7aac335a9d4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.8994137048721313,\n",
       " 'test_accuracy': 0.5,\n",
       " 'test_runtime': 4.2884,\n",
       " 'test_samples_per_second': 2.332,\n",
       " 'test_steps_per_second': 1.166}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = trainer.predict(datasets['test'])\n",
    "test_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(test_result, configuration['testing']['results_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_config.test_set_select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T19:12:03.464164Z",
     "start_time": "2023-04-19T19:12:03.090216Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.932342052459717,\n",
       " 'test_accuracy': 0.0,\n",
       " 'test_runtime': 4.6472,\n",
       " 'test_samples_per_second': 2.152,\n",
       " 'test_steps_per_second': 1.076}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_result = trainer.predict(datasets['control'])\n",
    "control_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(control_result, configuration['control']['results_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_config.control_set_select()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
